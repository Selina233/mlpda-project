{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "from utilities import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å®šä¹‰å‡ ä¸ªè¾…åŠ©å‡½æ•°ï¼š\n",
        "â†“ğŸ‘‡ä»¥ä¸‹æ˜¯ä¸¤ä¸ªè¡¨ï¼šå•è¯->å•è¯åºå·ï¼›å•è¯åºå·->è¯å‘é‡ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "word2indexï¼šæŠŠå•è¯è½¬æˆç´¢å¼•ï¼Œç´¢å¼•å¯ä»¥æ‰¾åˆ°è¯å‘é‡\n",
        "\n",
        "wocabulary_vectorsï¼šæ‰€æœ‰å•è¯çš„glove embeddingï¼Œç´¢å¼•æ˜¯å•è¯id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_list = []\n",
        "vocabulary_vectors = []\n",
        "#å¤§å®¶éƒ½ç”¨100d\n",
        "glove_data = open('data/glove.6B.100d.txt', encoding='utf-8') # æˆ‘ä¸‹ä¸‹æ¥çš„gloveæ˜¯100dï¼Œyytä½ è¦ç”¨çš„è¯è®°å¾—æ”¹å›ä¸Šä¸€è¡Œï¼ˆï¼‰\n",
        "for line in glove_data.readlines():\n",
        "    temp = line.strip('\\n').split(' ')  # ä¸€ä¸ªåˆ—è¡¨\n",
        "    name = temp[0]\n",
        "    word_list.append(name.lower())\n",
        "    vector = [temp[i] for i in range(1, len(temp))]  # å‘é‡\n",
        "    vector = list(map(float, vector))  # å˜æˆæµ®ç‚¹æ•°\n",
        "    vocabulary_vectors.append(vector)\n",
        "# ä¿å­˜\n",
        "vocabulary_vectors = np.array(vocabulary_vectors)\n",
        "word_list = np.array(word_list)\n",
        "np.save('npys/vocabulary_vectors', vocabulary_vectors)\n",
        "np.save('npys/word_list', word_list)\n",
        "#ä¿å­˜ä½ ğŸå‘¢ï¼Œé©¬ä¸Šå°±ç”¨äº†\n",
        "#ç¬¨ç¬¨\n",
        "word_list = np.load('npys/word_list.npy', allow_pickle=True)\n",
        "\n",
        "# ç›´æ¥tmç»™ä½ è½¬æˆå“ˆå¸Œè¡¨ï¼Œå‚»å­æ‰ç”¨listä¸€ä¸ªä¸€ä¸ªæœç´¢å‘¢ï¼Œå ªç§°å¤´éƒ¨èºæ—‹æ¡¨\n",
        "word_list = word_list.tolist()\n",
        "word2index={} # word->index\n",
        "for i in range(len(word_list)):\n",
        "    word2index[word_list[i]]=i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "â†“ğŸ‘‡è¿™ä¸ªç”¨æ¥ä»æ–‡ä»¶ä¸­åŠ è½½åŸå§‹æ–‡æœ¬æ•°æ®å¹¶å¤„ç†æˆå•è¯åˆ—è¡¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(path, flag='train'):\n",
        "    labels = ['pos', 'neg']\n",
        "    data = []\n",
        "    r = '[â€™!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\nã€‚ï¼ï¼Œ]+'\n",
        "    compiled = re.compile(r)\n",
        "    for label in labels:\n",
        "        files = os.listdir(os.path.join(path, flag, label))\n",
        "        for file in tqdm.tqdm(files): # Don't be nervous, tqdm is only a progress bar. \n",
        "            with open(os.path.join(path, flag, label, file), 'r', encoding='utf8') as rf:\n",
        "                temp = rf.read()\n",
        "                temp = temp.replace('\\n', ' ').replace('<br /><br />', ' ')\n",
        "                #åœ¨æ ‡ç‚¹ç¬¦å·å·¦å³åŠ ç©ºæ ¼ï¼Œä¸ºäº†è®©æ ‡ç‚¹ä¹Ÿç‹¬ç«‹æˆä¸ºå•è¯\n",
        "                temp = re.sub(r, ' \\g<0> ', temp)\n",
        "                temp = temp.split(' ')\n",
        "                temp = [temp[i].lower() for i in range(len(temp)) if temp[i] != '']\n",
        "                if label == 'pos':\n",
        "                    data.append([temp, 1])\n",
        "                elif label == 'neg':\n",
        "                    data.append([temp, 0])\n",
        "            #break # Remember to delete this\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "â†“ğŸ‘‡è¿™ä¸ªç”¨æ¥æŠŠæ‰€æœ‰æ–‡æœ¬è½¬æˆnumpyæ•°ç»„ï¼Œç„¶åå­˜åœ¨æ–‡ä»¶é‡Œã€‚è‡³æ­¤ï¼Œé¢„å¤„ç†å®Œæˆã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "SENTENCE_MAXLEN=600\n",
        "def process_sentence(flag: str, path: str = 'data/aclImdb', length_limit: int = SENTENCE_MAXLEN):\n",
        "    '''Process data into numpy arrays and save them. \n",
        "    ---\n",
        "    They look like: \n",
        "    \n",
        "    sentence_code: [[word IDs], [word IDs], ...]\n",
        "    \n",
        "    labels: [label, label, ...]\n",
        "    \n",
        "    flag should be either \"train\" or \"test\". \n",
        "    '''\n",
        "    output_dir = os.path.join(\"./npys\", flag)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    # if os.path.exists(os.path.join(output_dir, \"sentence_code.npy\")) and os.path.exists(os.path.join(output_dir, \"labels.npy\")):\n",
        "    #     print(\"å¤§å“¥ï¼Œä½ å·²ç»é¢„å¤„ç†è¿‡äº†ğŸ¤£\")\n",
        "    #     return []\n",
        "    \n",
        "    sentence_code = []\n",
        "    labels = []\n",
        "    reallen=[]\n",
        "    length=[0 for i in range(2000)] # ç»Ÿè®¡è¯„è®ºé•¿åº¦åˆ†å¸ƒ\n",
        "    test_data = load_data(path, flag)\n",
        "   \n",
        "    for i in tqdm.tqdm(range(len(test_data))): # Don't be nervous, tqdm is only a progress bar. \n",
        "        # nb\n",
        "        # print(i)\n",
        "        vec = test_data[i][0]\n",
        "        label = test_data[i][1] # 0 or 1 0 means neg 1 means positive\n",
        "        temp = []\n",
        "        index = 0\n",
        "        for j in range(len(vec)):\n",
        "            try:\n",
        "                index = word2index[vec[j]]\n",
        "            except KeyError:  # æ²¡æ‰¾åˆ°\n",
        "                index = -1 #400000 åœ¨ glove6Bé‡Œæ˜¯ <unk>çš„ index #ä¸¤ä¸ªgloveè¿˜ä¸ä¸€æ · æ— è¯­ éƒ½å˜æˆ-1å¥½äº†\n",
        "            finally:\n",
        "                temp.append(index)  # tempè¡¨ç¤ºä¸€ä¸ªå•è¯åœ¨è¯å…¸ä¸­çš„åºå·\n",
        "        \n",
        "        # ç»Ÿè®¡è¯„è®ºé•¿åº¦åˆ†å¸ƒ\n",
        "        l=len(temp)\n",
        "        if(l<10):\n",
        "            print(vec,label)\n",
        "        reallen.append(l)\n",
        "        \n",
        "        if l>=2000:\n",
        "            l=1999\n",
        "        length[l]+=1\n",
        "        \n",
        "\n",
        "        if l<length_limit:\n",
        "            for k in range(l, SENTENCE_MAXLEN):  # ä¸èƒ½è¡¥ 0 å› ä¸º 0 æ˜¯ the çš„ index è¿™é‡Œè¡¥ -1 è½¬æ¢æˆè¯å‘é‡æ—¶ç‰¹æ®Šå¤„ç†\n",
        "                temp.append(-1)\n",
        "        else:\n",
        "            temp = temp[0:SENTENCE_MAXLEN]  # åªä¿ç•™250ä¸ª\n",
        "        sentence_code.append(temp)\n",
        "        labels.append(label)\n",
        "     \n",
        "\n",
        "    sentence_code = np.array(sentence_code,dtype=object)\n",
        "    np.save(os.path.join(output_dir, \"sentence_code\"), sentence_code)\n",
        "    np.save(os.path.join(output_dir, \"labels\"), labels)\n",
        "    return length,reallen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "æµ‹è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹æ•°æ®å‘è‚²æ­£ä¸æ­£å¸¸å•Š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "å¤§å“¥ï¼Œä½ å·²ç»é¢„å¤„ç†è¿‡äº†ğŸ¤£ä¸è¿‡ä¸ºäº†ä¿é™©èµ·è§ï¼Œè¿˜æ˜¯é‡æ–°é¢„å¤„ç†ä¸€ä¸‹\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:02<00:00, 4250.26it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:02<00:00, 4180.41it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:01<00:00, 17739.09it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:02<00:00, 4558.27it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:02<00:00, 4285.98it/s]\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 22362/25000 [00:01<00:00, 18033.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['primary', 'plot', '!', 'primary', 'direction', '!', 'poor', 'interpretation', '.'] 0\n",
            "['read', 'the', 'book', ',', 'forget', 'the', 'movie', '!'] 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:01<00:00, 17704.23it/s]\n"
          ]
        }
      ],
      "source": [
        "process_sentence(word2index, \"train\")\n",
        "process_sentence(word2index, \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "['primary', 'plot', '!', 'primary', 'direction', '!', 'poor', 'interpretation', '.'] 0\n",
        "['read', 'the', 'book', ',', 'forget', 'the', 'movie', '!'] 0\n",
        "xswl çœ‹çœ‹æš´èºå½±è¯„"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸ‘‡çœ‹ä¸€ä¸‹æ•°æ®åˆ†å¸ƒ&ç”»å›¾å›¾"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.plot(result_train_label[0])\n",
        "# plt.title(\"Length distribution of negative instances\")\n",
        "# plt.show()\n",
        "# plt.plot(result_train_label[1])\n",
        "# plt.title(\"Length distribution of positive instances\")\n",
        "# plt.show()\n",
        "# # result_reallen_train.sort()\n",
        "# # IQR=result_reallen_train[(int)(len(result_reallen_train)*0.75)]-result_reallen_train[(int)(len(result_reallen_train)*0.25)]\n",
        "# # fuck=result_reallen_train[(int)(len(result_reallen_train)*0.75)]+1.5*IQR\n",
        "# # plt.boxplot(result_reallen_train)\n",
        "# # plt.show()\n",
        "# # plt.boxplot(result_reallen_test)\n",
        "# # plt.show()\n",
        "# words = sorted(word_cnt_train_label[0], key=lambda x: word_cnt_train_label[0][x], reverse=True)[:20]\n",
        "# plt.bar(words, [word_cnt_train_label[0][w] for w in words])\n",
        "# plt.title(\"Words distribution of negative instances\")\n",
        "# plt.show()\n",
        "# words = sorted(word_cnt_train_label[1], key=lambda x: word_cnt_train_label[1][x], reverse=True)[:20]\n",
        "# plt.bar(words, [word_cnt_train_label[1][w] for w in words])\n",
        "# plt.title(\"Words distribution of positive instances\")\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # print(fuck)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å”å”æˆ‘å•Šï¼Œè¦å¼€å§‹è®­ç»ƒäº†æ\n",
        "\n",
        "é¦–å…ˆï¼Œå†™ä¸€ä¸ªè·å–æ•°æ®çš„å‡½æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from model import MyTransformerEncoder\n",
        "# import dataset\n",
        "\n",
        "# model = MyTransformerEncoder()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.13 ('ML')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8809126b2e3f6bd67afd8dec0aaf136102c3339cf179547b748c69a78a732e29"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
