{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "from utilities import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 定义几个辅助函数：\n",
        "↓👇以下是两个表：单词->单词序号；单词序号->词向量。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "word2index：把单词转成索引，索引可以找到词向量\n",
        "\n",
        "wocabulary_vectors：所有单词的glove embedding，索引是单词id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_list = []\n",
        "vocabulary_vectors = []\n",
        "#大家都用100d\n",
        "glove_data = open('data/glove.6B.100d.txt', encoding='utf-8') # 我下下来的glove是100d，yyt你要用的话记得改回上一行（）\n",
        "for line in glove_data.readlines():\n",
        "    temp = line.strip('\\n').split(' ')  # 一个列表\n",
        "    name = temp[0]\n",
        "    word_list.append(name.lower())\n",
        "    vector = [temp[i] for i in range(1, len(temp))]  # 向量\n",
        "    vector = list(map(float, vector))  # 变成浮点数\n",
        "    vocabulary_vectors.append(vector)\n",
        "# 保存\n",
        "vocabulary_vectors = np.array(vocabulary_vectors)\n",
        "word_list = np.array(word_list)\n",
        "np.save('npys/vocabulary_vectors', vocabulary_vectors)\n",
        "np.save('npys/word_list', word_list)\n",
        "#保存你🐎呢，马上就用了\n",
        "#笨笨\n",
        "word_list = np.load('npys/word_list.npy', allow_pickle=True)\n",
        "\n",
        "# 直接tm给你转成哈希表，傻子才用list一个一个搜索呢，堪称头部螺旋桨\n",
        "word_list = word_list.tolist()\n",
        "word2index={} # word->index\n",
        "for i in range(len(word_list)):\n",
        "    word2index[word_list[i]]=i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "↓👇这个用来从文件中加载原始文本数据并处理成单词列表"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(path, flag='train'):\n",
        "    labels = ['pos', 'neg']\n",
        "    data = []\n",
        "    r = '[’!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n。！，]+'\n",
        "    compiled = re.compile(r)\n",
        "    for label in labels:\n",
        "        files = os.listdir(os.path.join(path, flag, label))\n",
        "        for file in tqdm.tqdm(files): # Don't be nervous, tqdm is only a progress bar. \n",
        "            with open(os.path.join(path, flag, label, file), 'r', encoding='utf8') as rf:\n",
        "                temp = rf.read()\n",
        "                temp = temp.replace('\\n', ' ').replace('<br /><br />', ' ')\n",
        "                #在标点符号左右加空格，为了让标点也独立成为单词\n",
        "                temp = re.sub(r, ' \\g<0> ', temp)\n",
        "                temp = temp.split(' ')\n",
        "                temp = [temp[i].lower() for i in range(len(temp)) if temp[i] != '']\n",
        "                if label == 'pos':\n",
        "                    data.append([temp, 1])\n",
        "                elif label == 'neg':\n",
        "                    data.append([temp, 0])\n",
        "            #break # Remember to delete this\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "↓👇这个用来把所有文本转成numpy数组，然后存在文件里。至此，预处理完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "SENTENCE_MAXLEN=600\n",
        "def process_sentence(flag: str, path: str = 'data/aclImdb', length_limit: int = SENTENCE_MAXLEN):\n",
        "    '''Process data into numpy arrays and save them. \n",
        "    ---\n",
        "    They look like: \n",
        "    \n",
        "    sentence_code: [[word IDs], [word IDs], ...]\n",
        "    \n",
        "    labels: [label, label, ...]\n",
        "    \n",
        "    flag should be either \"train\" or \"test\". \n",
        "    '''\n",
        "    output_dir = os.path.join(\"./npys\", flag)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    # if os.path.exists(os.path.join(output_dir, \"sentence_code.npy\")) and os.path.exists(os.path.join(output_dir, \"labels.npy\")):\n",
        "    #     print(\"大哥，你已经预处理过了🤣\")\n",
        "    #     return []\n",
        "    \n",
        "    sentence_code = []\n",
        "    labels = []\n",
        "    reallen=[]\n",
        "    length=[0 for i in range(2000)] # 统计评论长度分布\n",
        "    test_data = load_data(path, flag)\n",
        "   \n",
        "    for i in tqdm.tqdm(range(len(test_data))): # Don't be nervous, tqdm is only a progress bar. \n",
        "        # nb\n",
        "        # print(i)\n",
        "        vec = test_data[i][0]\n",
        "        label = test_data[i][1] # 0 or 1 0 means neg 1 means positive\n",
        "        temp = []\n",
        "        index = 0\n",
        "        for j in range(len(vec)):\n",
        "            try:\n",
        "                index = word2index[vec[j]]\n",
        "            except KeyError:  # 没找到\n",
        "                index = -1 #400000 在 glove6B里是 <unk>的 index #两个glove还不一样 无语 都变成-1好了\n",
        "            finally:\n",
        "                temp.append(index)  # temp表示一个单词在词典中的序号\n",
        "        \n",
        "        # 统计评论长度分布\n",
        "        l=len(temp)\n",
        "        if(l<10):\n",
        "            print(vec,label)\n",
        "        reallen.append(l)\n",
        "        \n",
        "        if l>=2000:\n",
        "            l=1999\n",
        "        length[l]+=1\n",
        "        \n",
        "\n",
        "        if l<length_limit:\n",
        "            for k in range(l, SENTENCE_MAXLEN):  # 不能补 0 因为 0 是 the 的 index 这里补 -1 转换成词向量时特殊处理\n",
        "                temp.append(-1)\n",
        "        else:\n",
        "            temp = temp[0:SENTENCE_MAXLEN]  # 只保留250个\n",
        "        sentence_code.append(temp)\n",
        "        labels.append(label)\n",
        "     \n",
        "\n",
        "    sentence_code = np.array(sentence_code,dtype=object)\n",
        "    np.save(os.path.join(output_dir, \"sentence_code\"), sentence_code)\n",
        "    np.save(os.path.join(output_dir, \"labels\"), labels)\n",
        "    return length,reallen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "测试一下，看看数据发育正不正常啊"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "大哥，你已经预处理过了🤣不过为了保险起见，还是重新预处理一下\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12500/12500 [00:02<00:00, 4250.26it/s]\n",
            "100%|██████████| 12500/12500 [00:02<00:00, 4180.41it/s]\n",
            "100%|██████████| 25000/25000 [00:01<00:00, 17739.09it/s]\n",
            "100%|██████████| 12500/12500 [00:02<00:00, 4558.27it/s]\n",
            "100%|██████████| 12500/12500 [00:02<00:00, 4285.98it/s]\n",
            " 89%|████████▉ | 22362/25000 [00:01<00:00, 18033.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['primary', 'plot', '!', 'primary', 'direction', '!', 'poor', 'interpretation', '.'] 0\n",
            "['read', 'the', 'book', ',', 'forget', 'the', 'movie', '!'] 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 25000/25000 [00:01<00:00, 17704.23it/s]\n"
          ]
        }
      ],
      "source": [
        "process_sentence(word2index, \"train\")\n",
        "process_sentence(word2index, \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "['primary', 'plot', '!', 'primary', 'direction', '!', 'poor', 'interpretation', '.'] 0\n",
        "['read', 'the', 'book', ',', 'forget', 'the', 'movie', '!'] 0\n",
        "xswl 看看暴躁影评"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "👇看一下数据分布&画图图"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.plot(result_train_label[0])\n",
        "# plt.title(\"Length distribution of negative instances\")\n",
        "# plt.show()\n",
        "# plt.plot(result_train_label[1])\n",
        "# plt.title(\"Length distribution of positive instances\")\n",
        "# plt.show()\n",
        "# # result_reallen_train.sort()\n",
        "# # IQR=result_reallen_train[(int)(len(result_reallen_train)*0.75)]-result_reallen_train[(int)(len(result_reallen_train)*0.25)]\n",
        "# # fuck=result_reallen_train[(int)(len(result_reallen_train)*0.75)]+1.5*IQR\n",
        "# # plt.boxplot(result_reallen_train)\n",
        "# # plt.show()\n",
        "# # plt.boxplot(result_reallen_test)\n",
        "# # plt.show()\n",
        "# words = sorted(word_cnt_train_label[0], key=lambda x: word_cnt_train_label[0][x], reverse=True)[:20]\n",
        "# plt.bar(words, [word_cnt_train_label[0][w] for w in words])\n",
        "# plt.title(\"Words distribution of negative instances\")\n",
        "# plt.show()\n",
        "# words = sorted(word_cnt_train_label[1], key=lambda x: word_cnt_train_label[1][x], reverse=True)[:20]\n",
        "# plt.bar(words, [word_cnt_train_label[1][w] for w in words])\n",
        "# plt.title(\"Words distribution of positive instances\")\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # print(fuck)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 叔叔我啊，要开始训练了捏\n",
        "\n",
        "首先，写一个获取数据的函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from model import MyTransformerEncoder\n",
        "# import dataset\n",
        "\n",
        "# model = MyTransformerEncoder()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.13 ('ML')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8809126b2e3f6bd67afd8dec0aaf136102c3339cf179547b748c69a78a732e29"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
