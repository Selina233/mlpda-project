'''
è¿™æ˜¯ä¸€äº›åŠŸèƒ½å‡½æ•°ã€‚
ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘æŠŠä¸€äº›å•è¯å¤„ç†çš„å‡½æ•°æ¬åˆ°è¿™é‡Œæ¥äº†ã€‚
'''
import os
import re

import numpy as np
import tqdm

SENTENCE_MAXLEN=250

def text_2_encoding(sentence: str, word2index: dict, vocabulary_vectors, length_limit: int = SENTENCE_MAXLEN):
    '''
    æŠŠä¸€ä¸ªå…¨æ˜¯word idçš„åˆ—è¡¨å˜æˆglove encodingå‘é‡ï¼Œç”¨äºè¾“å…¥æ¨¡å‹ã€‚
    '''
    sentence = process_single_sentence(sentence, word2index, length_limit)
    return [vocabulary_vectors[word_id] if word_id>=0 else np.zeros(100) for word_id in sentence]
    

def get_glove_encoding(glove_data_path: str = 'data/glove.6B.100d.txt'):
    '''
    è·å¾—gloveé‡Œé¢çš„å•è¯ç¼–ç ã€‚
    
    è¿”å›ï¼šword2indexï¼ˆå•è¯æ˜ å°„åˆ°å•è¯idçš„å­—å…¸ï¼‰ï¼Œ
        vocabulary_vectorsï¼ˆæ¯ä¸ªå•è¯çš„embedding vectorç»„æˆçš„åˆ—è¡¨ï¼Œç”¨å•è¯çš„idä½œä¸ºä¸‹æ ‡æ¥è®¿é—®ï¼‰
    '''
    # è¯»å–æ–‡æœ¬æ–‡ä»¶
    glove_data = open(glove_data_path, encoding='utf-8')
    
    # æŠŠå¥å­è½¬æˆidå‘é‡
    word_list = []
    vocabulary_vectors = []
    for line in glove_data.readlines():
        temp = line.strip('\n').split(' ')  # ä¸€ä¸ªåˆ—è¡¨
        name = temp[0]
        word_list.append(name.lower())
        vector = [temp[i] for i in range(1, len(temp))]  # å‘é‡
        vector = list(map(float, vector))  # å˜æˆæµ®ç‚¹æ•°
        vocabulary_vectors.append(vector)
        
    vocabulary_vectors = np.array(vocabulary_vectors)

    # ç›´æ¥tmç»™ä½ è½¬æˆå“ˆå¸Œè¡¨ï¼Œå‚»å­æ‰ç”¨listä¸€ä¸ªä¸€ä¸ªæœç´¢å‘¢ï¼Œå ªç§°å¤´éƒ¨èºæ—‹æ¡¨
    word2index = {} # word->index
    for i in range(len(word_list)):
        word2index[word_list[i]]=i
    
    np.save("./npys/vocabulary_vectors.npy", vocabulary_vectors, allow_pickle=True)
    np.save("./npys/word2index.npy", word2index, allow_pickle=True)
    return word2index, vocabulary_vectors


########################################################################################
# ä»¥ä¸‹æ˜¯å¤„ç†æ•°æ®çš„4ä¸ªå‡½æ•°ï¼Œåˆ†åˆ«æ˜¯ï¼šåˆ†å‰²å¥å­ä¸­çš„å•è¯ï¼Œå¤„ç†ä¸€ä¸ªå¥å­ï¼Œå¤„ç†æ‰€æœ‰æ–‡ä»¶é‡Œçš„å¥å­ï¼ŒæŠŠæ‰€æœ‰æ•°æ®æ‰“åŒ…å­˜èµ·æ¥

def sentence_split(sentence: str):
    '''
    Process a single sentence to [word_ids]. 
    Returns: processed [word_ids], and [words] that is original length. 
    '''
    r = '[â€™!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~\nã€‚ï¼ï¼Œ]+'
    sentence = sentence.replace('\n', ' ').replace('<br /><br />', ' ')
    sentence = re.sub(r, ' \g<0> ', sentence) # åœ¨æ ‡ç‚¹ç¬¦å·å·¦å³åŠ ç©ºæ ¼ï¼Œä¸ºäº†è®©æ ‡ç‚¹ä¹Ÿç‹¬ç«‹æˆä¸ºå•è¯
    sentence = sentence.split(' ')
    words = [sentence[i].lower() for i in range(len(sentence)) if sentence[i] != '']
    return words

def process_single_sentence(sentence: str, word2index: dict, length_limit: int = SENTENCE_MAXLEN, debug=False):
    '''
    Process a single sentence to [word_ids]. 
    Returns: processed [word_ids], and [words] that is original length. 
    '''
    # åˆ†å‰²å¥å­ä¸­çš„å•è¯
    words = sentence_split(sentence)
    
    # æŠŠä¸€ä¸ªå…¨æ˜¯å°å†™è‹±æ–‡å•è¯ï¼ˆæˆ–æ ‡ç‚¹ç¬¦å·ï¼‰çš„listè½¬æ¢æˆå•è¯idçš„listã€‚
    temp = []
    index = -114514
    for j in range(len(words)):
        try:
            index = word2index[words[j]]
        except KeyError:  # æ²¡æ‰¾åˆ°
            index = 400000 # 400000 åœ¨ glove6Bé‡Œæ˜¯ <unk>çš„ index
        finally:
            temp.append(index)  # indexè¡¨ç¤ºä¸€ä¸ªå•è¯åœ¨è¯å…¸ä¸­çš„id
            
    # å¤„ç†æˆè§„å®šé•¿åº¦
    for i in range(len(temp), length_limit):  # ä¸èƒ½è¡¥ 0 å› ä¸º 0 æ˜¯ the çš„ index, è¿™é‡Œè¡¥ -1 è½¬æ¢æˆè¯å‘é‡æ—¶ç‰¹æ®Šå¤„ç†
        temp.append(-1)
    if len(temp) > length_limit:
        temp = temp[0:length_limit]  # åªä¿ç•™length_limitä¸ª
    return temp


def load_data(path, word2index, flag='train', length_limit: int = SENTENCE_MAXLEN):
    '''
    Open data from files and process all txts into [[[word IDs], label], [[word IDs], label], ...]. 
    '''
    labels = ['pos', 'neg']
    data = []
    for label in labels:
        files = os.listdir(os.path.join(path, flag, label))
        for file in tqdm.tqdm(files):
            with open(os.path.join(path, flag, label, file), 'r', encoding='utf8') as rf:
                temp = rf.read()
                temp = process_single_sentence(temp, word2index, length_limit)
                if label == 'pos':
                    data.append([temp, 1])
                elif label == 'neg':
                    data.append([temp, 0])
    return data


def process_sentence(word2index: dict, flag: str, path: str = 'data/aclImdb', length_limit: int = SENTENCE_MAXLEN):
    '''Process data into numpy arrays and save them. 
    ---
    They look like: 
    
    sentence_code: [[word IDs], [word IDs], ...]
    
    labels: [label, label, ...]
    
    flag should be either "train" or "test". 
    '''
    output_dir = os.path.join("./npys", flag)
    os.makedirs(output_dir, exist_ok=True)
    if os.path.exists(os.path.join(output_dir, "sentence_code.npy")) and os.path.exists(os.path.join(output_dir, "labels.npy")):
        print("å¤§å“¥ï¼Œä½ å·²ç»é¢„å¤„ç†è¿‡äº†ğŸ¤£ä¸è¿‡ä¸ºäº†ä¿é™©èµ·è§ï¼Œè¿˜æ˜¯é‡æ–°é¢„å¤„ç†ä¸€ä¸‹")
    
    sentence_code = []
    labels = []
    test_data = load_data(path, word2index, flag, length_limit)
    
    for i in tqdm.tqdm(range(len(test_data))):
        # nb
        # print(i)
        temp = test_data[i][0]
        label = test_data[i][1] # 0 or 1 0 means neg 1 means positive

        sentence_code.append(temp)
        labels.append(label)
     

    sentence_code = np.array(sentence_code)
    np.save(os.path.join(output_dir, "sentence_code"), sentence_code)
    np.save(os.path.join(output_dir, "labels"), labels)
    print(sentence_code[:5])
    print(labels[:5])
    
# æ•°æ®å¤„ç†åˆ°æ­¤å®Œæˆã€‚
########################################################################################